{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BabakBagheriGisour2/Final-Assignment.ipynb/blob/main/Kopie_von_SIC_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7xU_zniZEzZ",
        "outputId": "4114d327-5269-4a2f-f51e-38e1b8888a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.8.30)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21692 sha256=e5a66bdee9fb96db2d2e025d339ac1d5a485c35b635a8cae08209c3f3a910142\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=aaaac84b2cae247718a9fefa025f67779ac8b7896a9fbec1b775110bfd70c0f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=77e95fa8adb1beb9d9b176447f36aacd42eabb68f5cf0e6d8660aaf4019b45c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: Google in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from Google) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->Google) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install sumy\n",
        "!pip install transformers\n",
        "!pip install rouge_score\n",
        "!pip install sentencepiece\n",
        "!pip install torch\n",
        "!pip install Google\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# اتصال به Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGxO-sgjQVSC",
        "outputId": "f2274206-54fa-4a5e-e968-5159eca917d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**تبدیل پی دی اف به جیسون لاین**"
      ],
      "metadata": {
        "id": "DvpZtlyR2F5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import fitz  # PyMuPDF\n",
        "from tqdm import tqdm\n",
        "\n",
        "# مسیرهای ورودی و خروجی\n",
        "input_folder = '/content/'\n",
        "output_folder = '/content/output'\n",
        "\n",
        "# اطمینان حاصل می‌کنیم که پوشه خروجی وجود دارد\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# بارگذاری مدل و توکنایزر برای خلاصه‌سازی با facebook/bart-large-cnn\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# بررسی اینکه آیا GPU موجود است یا خیر\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# تابعی برای استخراج متن از PDF و ایجاد خلاصه با BART\n",
        "def extract_text_and_summary(pdf_path):\n",
        "    # باز کردن فایل PDF\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    # لیست برای ذخیره داده‌ها\n",
        "    data = []\n",
        "\n",
        "    # پردازش هر صفحه\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc.load_page(page_num)\n",
        "        text = page.get_text(\"text\")  # استخراج متن صفحه\n",
        "\n",
        "        # خلاصه‌سازی متن با استفاده از مدل BART\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
        "        inputs = inputs.to(device)\n",
        "        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
        "\n",
        "        # تبدیل IDهای خلاصه به متن\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # اضافه کردن داده‌ها به لیست\n",
        "        data.append({\n",
        "            \"text\": text,\n",
        "            \"zusammenfassen\": summary\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "# خواندن فایل‌های PDF از پوشه ورودی و ذخیره آنها در پوشه خروجی\n",
        "for filename in tqdm(os.listdir(input_folder)):\n",
        "    if filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(input_folder, filename)\n",
        "\n",
        "        # استخراج متن و خلاصه برای فایل PDF\n",
        "        data = extract_text_and_summary(pdf_path)\n",
        "\n",
        "        # نام فایل خروجی JSONL\n",
        "        output_filename = os.path.splitext(filename)[0] + '.jsonl'\n",
        "        output_path = os.path.join(output_folder, output_filename)\n",
        "\n",
        "        # ذخیره داده‌ها در فایل JSONL\n",
        "        with open(output_path, 'w', encoding='utf-8') as jsonl_file:\n",
        "            for item in data:\n",
        "                jsonl_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(\"تمامی فایل‌ها به JSONL تبدیل شدند.\")\n"
      ],
      "metadata": {
        "id": "BsLy1Fi4kBpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**آموزش مدل**"
      ],
      "metadata": {
        "id": "ipzhEPPJyWk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from google.colab import drive\n",
        "\n",
        "# اتصال به گوگل درایو\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# مسیر پوشه حاوی فایل‌های JSONL\n",
        "jsonl_folder = '/content/output/'\n",
        "\n",
        "# بارگذاری داده‌های JSONL\n",
        "def load_jsonl_data(jsonl_folder):\n",
        "    data = []\n",
        "    for filename in os.listdir(jsonl_folder):\n",
        "        if filename.endswith('.jsonl'):\n",
        "            file_path = os.path.join(jsonl_folder, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    entry = json.loads(line)\n",
        "                    data.append({\n",
        "                        'text': entry['text'],\n",
        "                        'summary': entry['zusammenfassen']\n",
        "                    })\n",
        "    return data\n",
        "\n",
        "# بارگذاری داده‌ها\n",
        "data = load_jsonl_data(jsonl_folder)\n",
        "\n",
        "# ایجاد دیتاست از داده‌ها\n",
        "dataset = Dataset.from_dict({\n",
        "    'text': [entry['text'] for entry in data],\n",
        "    'summary': [entry['summary'] for entry in data]\n",
        "})\n",
        "\n",
        "# بارگذاری مدل و توکنایزر BART\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# توکنایز کردن داده‌ها\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=1024)\n",
        "    targets = tokenizer(examples['summary'], padding=\"max_length\", truncation=True, max_length=150)\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# پیش‌پردازش داده‌ها\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# تنظیمات آموزشی\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # خروجی مدل\n",
        "    evaluation_strategy=\"epoch\",     # ارزیابی در هر دوره\n",
        "    learning_rate=5e-5,              # نرخ یادگیری\n",
        "    per_device_train_batch_size=4,   # اندازه بچ هر دستگاه\n",
        "    per_device_eval_batch_size=8,    # اندازه بچ برای ارزیابی\n",
        "    num_train_epochs=3,              # تعداد دوره‌های آموزش\n",
        "    weight_decay=0.01,               # کاهش وزن\n",
        "    logging_dir='./logs',            # مسیر لاگ‌ها\n",
        "    logging_steps=10,                # فواصل زمانی لاگ‌گذاری\n",
        ")\n",
        "\n",
        "# استفاده از Trainer برای آموزش مدل\n",
        "trainer = Trainer(\n",
        "    model=model,                       # مدل BART\n",
        "    args=training_args,                # تنظیمات آموزشی\n",
        "    train_dataset=tokenized_dataset,   # داده‌های آموزشی\n",
        "    eval_dataset=tokenized_dataset     # داده‌های ارزیابی\n",
        ")\n",
        "\n",
        "# آموزش مدل\n",
        "trainer.train()\n",
        "\n",
        "# ذخیره مدل در گوگل درایو\n",
        "drive_model_path = '/content/drive/MyDrive/final_model'  # مسیر ذخیره مدل در گوگل درایو\n",
        "model.save_pretrained(drive_model_path)\n",
        "tokenizer.save_pretrained(drive_model_path)\n",
        "\n",
        "print(\"Training complete and model saved to Google Drive.\")\n"
      ],
      "metadata": {
        "id": "OhqchaXUucUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ارزیابی مدل**"
      ],
      "metadata": {
        "id": "v8CKK8dlyKBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from datasets import load_metric\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "# اتصال به گوگل درایو و بارگذاری مدل و توکنایزر از درایو\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# مسیر مدل ذخیره‌شده در گوگل درایو\n",
        "drive_model_path = '/content/drive/MyDrive/final_model'\n",
        "\n",
        "# بارگذاری مدل و توکنایزر از گوگل درایو (مدل آموزش‌دیده)\n",
        "model = BartForConditionalGeneration.from_pretrained(drive_model_path)\n",
        "tokenizer = BartTokenizer.from_pretrained(drive_model_path)\n",
        "\n",
        "# مسیر فایل JSON Line داده‌های اعتبارسنجی\n",
        "validation_file_path = \"validation_data.jsonl\"\n",
        "\n",
        "# بارگذاری معیار BLEU برای ارزیابی\n",
        "bleu_metric = load_metric(\"bleu\")\n",
        "\n",
        "# لیست برای نگهداری ورودی‌ها و خروجی‌های واقعی و پیش‌بینی‌شده\n",
        "true_texts = []\n",
        "predicted_texts = []\n",
        "bleu_scores_per_sample = []\n",
        "\n",
        "# خواندن داده‌ها از فایل JSON Line و انجام پیش‌بینی مدل\n",
        "with open(validation_file_path, 'r') as file:\n",
        "    for i, line in enumerate(file):\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # فرض می‌کنیم که ورودی‌ها در \"input_text\" و خروجی‌های واقعی در \"target_text\" هستند\n",
        "        input_text = data[\"input_text\"]\n",
        "        target_text = data[\"target_text\"]\n",
        "\n",
        "        # توکنایز کردن ورودی و تولید پاسخ با استفاده از مدل آموزش‌دیده BART\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        output_ids = model.generate(inputs['input_ids'], max_length=150, num_beams=4, early_stopping=True)\n",
        "        predicted_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # افزودن خروجی‌های واقعی و پیش‌بینی‌شده به لیست‌ها\n",
        "        true_texts.append([target_text.split()])  # BLEU نیاز به آرایه‌های توکنیزه شده دارد\n",
        "        predicted_texts.append(predicted_text.split())\n",
        "\n",
        "        # محاسبه BLEU برای هر نمونه و اضافه کردن آن به لیست\n",
        "        sample_bleu_score = bleu_metric.compute(predictions=[predicted_text.split()], references=[[target_text.split()]])[\"bleu\"]\n",
        "        bleu_scores_per_sample.append(sample_bleu_score)\n",
        "\n",
        "        # چاپ اطلاعات هر نمونه\n",
        "        print(f\"Sample {i+1}\")\n",
        "        print(\"Input Text: \", input_text)\n",
        "        print(\"Target Text: \", target_text)\n",
        "        print(\"Predicted Text: \", predicted_text)\n",
        "        print(\"BLEU Score for this sample: \", sample_bleu_score)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# محاسبه امتیاز BLEU کلی برای کل داده‌ها\n",
        "overall_bleu_score = bleu_metric.compute(predictions=predicted_texts, references=true_texts)\n",
        "\n",
        "# نمایش BLEU کلی\n",
        "print(\"\\nOverall BLEU Score:\", overall_bleu_score[\"bleu\"])\n",
        "\n",
        "# رسم نمودار BLEU برای نمونه‌ها\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(bleu_scores_per_sample, marker='o', linestyle='-', color='b')\n",
        "plt.title(\"BLEU Score per Sample\")\n",
        "plt.xlabel(\"Sample Number\")\n",
        "plt.ylabel(\"BLEU Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xZn_JMmiw1cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**تست مدل**"
      ],
      "metadata": {
        "id": "_1G37cEIyFsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from datasets import load_metric\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# مسیر مدل ذخیره‌شده در گوگل درایو\n",
        "drive_model_path = '/content/drive/MyDrive/final_model'\n",
        "model = BartForConditionalGeneration.from_pretrained(drive_model_path)\n",
        "tokenizer = BartTokenizer.from_pretrained(drive_model_path)\n",
        "\n",
        "# مسیر فایل JSON Line داده‌های تست\n",
        "test_file_path = \"test_data.jsonl\"\n",
        "\n",
        "# بارگذاری معیار BLEU برای ارزیابی\n",
        "bleu_metric = load_metric(\"bleu\")\n",
        "\n",
        "# لیست برای نگهداری ورودی‌ها و خروجی‌های واقعی و پیش‌بینی‌شده\n",
        "true_texts = []\n",
        "predicted_texts = []\n",
        "bleu_scores_per_sample = []\n",
        "\n",
        "# خواندن داده‌ها از فایل JSON Line و انجام پیش‌بینی مدل\n",
        "with open(test_file_path, 'r') as file:\n",
        "    for i, line in enumerate(file):\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # فرض می‌کنیم که ورودی‌ها در \"input_text\" و خروجی‌های واقعی در \"target_text\" هستند\n",
        "        input_text = data[\"input_text\"]\n",
        "        target_text = data[\"target_text\"]\n",
        "\n",
        "        # توکنایز کردن ورودی و تولید پاسخ با استفاده از مدل BART\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        output_ids = model.generate(inputs['input_ids'], max_length=150, num_beams=4, early_stopping=True)\n",
        "        predicted_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # افزودن خروجی‌های واقعی و پیش‌بینی‌شده به لیست‌ها\n",
        "        true_texts.append([target_text.split()])  # BLEU نیاز به آرایه‌های توکنیزه شده دارد\n",
        "        predicted_texts.append(predicted_text.split())\n",
        "\n",
        "        # محاسبه BLEU برای هر نمونه و اضافه کردن آن به لیست\n",
        "        sample_bleu_score = bleu_metric.compute(predictions=[predicted_text.split()], references=[[target_text.split()]])[\"bleu\"]\n",
        "        bleu_scores_per_sample.append(sample_bleu_score)\n",
        "\n",
        "        # چاپ اطلاعات هر نمونه\n",
        "        print(f\"Sample {i+1}\")\n",
        "        print(\"Input Text: \", input_text)\n",
        "        print(\"Target Text: \", target_text)\n",
        "        print(\"Predicted Text: \", predicted_text)\n",
        "        print(\"BLEU Score for this sample: \", sample_bleu_score)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# محاسبه امتیاز BLEU کلی برای کل داده‌ها\n",
        "overall_bleu_score = bleu_metric.compute(predictions=predicted_texts, references=true_texts)\n",
        "\n",
        "# نمایش BLEU کلی\n",
        "print(\"\\nOverall BLEU Score:\", overall_bleu_score[\"bleu\"])\n",
        "\n",
        "# رسم نمودار BLEU برای نمونه‌ها\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(bleu_scores_per_sample, marker='o', linestyle='-', color='b')\n",
        "plt.title(\"BLEU Score per Sample\")\n",
        "plt.xlabel(\"Sample Number\")\n",
        "plt.ylabel(\"BLEU Score\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# رسم هیستوگرام برای توزیع امتیازات BLEU\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(bleu_scores_per_sample, bins=20, color='purple', alpha=0.7)\n",
        "plt.title(\"BLEU Score Distribution\")\n",
        "plt.xlabel(\"BLEU Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-4-AgjC1xWF3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNLrLwgKfbh78p353ADgAYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}